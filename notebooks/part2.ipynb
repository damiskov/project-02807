{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d80775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# sys.path.append(str(Path(__file__).parent.parent))\n",
    "sys.path.append(\"..\")\n",
    "from utils.load import load_embeddings_dataset, clean_metadata_column\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "from graphviz import Graph\n",
    "import igraph\n",
    "\n",
    "igraph.config[\"plotting.backend\"] = \"matplotlib\"\n",
    "from igraph import Graph, plot\n",
    "from utils.graph_visualization import visualize_graph\n",
    "from utils.graph_clustering import spectral_clustering, to_knn_similarity, reduce_dim\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.internal_validation_metrics import manual_silhouette_score\n",
    "\n",
    "# from utils.external_validation import plot_merged_attribute_heatmap\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import ast\n",
    "from scipy.stats import chi2\n",
    "from utils.kl_matrix import create_kl_adjacency\n",
    "from utils.internal_validation_metrics import (\n",
    "    manual_silhouette_score,\n",
    "    manual_dbi,\n",
    "    manual_average_dispersion,\n",
    ")\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e92fe",
   "metadata": {},
   "source": [
    "# CTM-based Graph Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9e8986",
   "metadata": {},
   "source": [
    "### Load parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940abf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet with PyArrow, keeping pandas metadata\n",
    "table = pq.read_table(\"../data/sequence_dataset.parquet\", use_pandas_metadata=True)\n",
    "df = table.to_pandas()\n",
    "display(df.head(5))\n",
    "\n",
    "\n",
    "# Robust CTM stacking that unwraps singleton nesting and coerces to float array\n",
    "def stack_ctm(entry):\n",
    "    import ast\n",
    "\n",
    "    e = entry\n",
    "    # unwrap extra singleton nesting like [[...]] -> [...]\n",
    "    while isinstance(e, (list, tuple)) and len(e) == 1:\n",
    "        e = e[0]\n",
    "\n",
    "    # if stored as a string, try to parse it\n",
    "    if isinstance(e, str):\n",
    "        e = ast.literal_eval(e)\n",
    "\n",
    "    arr = np.array(e, dtype=object)\n",
    "\n",
    "    # if object dtype, try to build numeric rows\n",
    "    if arr.dtype == object:\n",
    "        try:\n",
    "            rows = [np.asarray(r, dtype=float) for r in arr]\n",
    "            arr = np.vstack(rows)\n",
    "        except Exception as exc:\n",
    "            raise ValueError(f\"Cannot convert CTM entry to numeric array: {exc}\")\n",
    "\n",
    "    # handle an extra leading singleton dimension (1,129,129)\n",
    "    if arr.ndim == 3 and arr.shape[0] == 1 and arr.shape[1:] == (129, 129):\n",
    "        arr = arr.squeeze(0)\n",
    "\n",
    "    if arr.shape != (129, 129):\n",
    "        raise ValueError(f\"Unexpected CTM shape {arr.shape}; expected (129,129)\")\n",
    "\n",
    "    return arr.astype(float)\n",
    "\n",
    "\n",
    "def normalize_global(M: np.ndarray) -> np.ndarray:\n",
    "    total = M.sum()\n",
    "    return M / total if total != 0 else M.astype(float)\n",
    "\n",
    "\n",
    "# Apply conversion + normalization with index-aware errors for easier debugging\n",
    "ctm_matrices = []\n",
    "for i, e in enumerate(df[\"ctm\"]):\n",
    "    try:\n",
    "        mat = stack_ctm(e)\n",
    "        mat = normalize_global(mat)\n",
    "        ctm_matrices.append(mat)\n",
    "    except Exception as exc:\n",
    "        raise ValueError(f\"Error processing ctm at row {i}: {exc}\")\n",
    "\n",
    "df[\"ctm_matrix\"] = ctm_matrices\n",
    "\n",
    "# Quick verification\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Example shape (row 0):\", df.loc[0, \"ctm_matrix\"].shape)\n",
    "print(\"Sum of example (row 0):\", df.loc[0, \"ctm_matrix\"].sum())\n",
    "# Optionally check that every matrix sums to 1 (or 0 if originally all zeros)\n",
    "sums = df[\"ctm_matrix\"].apply(np.sum)\n",
    "print(\"Number of matrices with sum == 1:\", (np.isclose(sums, 1.0)).sum())\n",
    "print(\"Number of zero-sum matrices:\", (sums == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5791b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(\"../data\") / \"ctms_distance_matrix.npy\"\n",
    "\n",
    "if p.exists():\n",
    "    print(\"Distance matrix already exists; loading from file.\")\n",
    "    distance_matrix = np.load(p)\n",
    "else:\n",
    "    ctms_list = df[\"ctm_matrix\"].tolist()\n",
    "    distance_matrix = create_kl_adjacency(ctms_list)\n",
    "    np.save(p, distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce4190d",
   "metadata": {},
   "source": [
    "## Spectral clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438dd2aa",
   "metadata": {},
   "source": [
    "### Distances to similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6493a",
   "metadata": {},
   "source": [
    "Compute a locally scaled similarity matrix using k-NN distances and adaptive Gaussian kernels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e614d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "def distance_to_local_scaled_similarity(\n",
    "    D: np.ndarray, k: int = 15, scale_factor: float = 2.0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Robust Self-Tuning Spectral Clustering.\n",
    "    1. Uses mean distance for sigma (prevents tiny sigma from duplicates).\n",
    "    2. Adds a scale_factor to widen the Gaussian kernel.\n",
    "    \"\"\"\n",
    "    n = D.shape[0]\n",
    "\n",
    "    # 1. k-NN search\n",
    "    # Request k+1 because the first neighbor is self (dist=0)\n",
    "    nbrs = NearestNeighbors(n_neighbors=k + 1, metric=\"precomputed\")\n",
    "    nbrs.fit(D)\n",
    "    knn_dists, knn_indices = nbrs.kneighbors(D)\n",
    "\n",
    "    # 2. Calculate ROBUST Sigmas\n",
    "    # Instead of just the k-th neighbor, use the MEAN of the k neighbors.\n",
    "    # This prevents 'collapsing' if the first few neighbors are duplicates.\n",
    "    # scale_factor (e.g. 2.0) ensures the tail of the distribution doesn't vanish too fast.\n",
    "    sigmas = np.mean(knn_dists[:, 1:], axis=1) * scale_factor\n",
    "\n",
    "    # Safety: prevent division by zero if all neighbors are identical\n",
    "    avg_global_dist = np.mean(knn_dists[:, 1:]) + 1e-9\n",
    "    sigmas[sigmas < 1e-9] = avg_global_dist\n",
    "\n",
    "    sim_matrix = np.zeros((n, n))\n",
    "\n",
    "    # 3. Compute Weights\n",
    "    for i in range(n):\n",
    "        neighbors = knn_indices[i, 1:]\n",
    "\n",
    "        # Original squared distances\n",
    "        d_sq = knn_dists[i, 1:] ** 2\n",
    "\n",
    "        # Scaling denominator: sigma_i * sigma_j\n",
    "        # This is the Zelnik-Manor magic: it adapts to the density of BOTH points.\n",
    "        sigma_prod = sigmas[i] * sigmas[neighbors]\n",
    "\n",
    "        # Compute Gaussian\n",
    "        weights = np.exp(-d_sq / sigma_prod)\n",
    "\n",
    "        # Hard check: If weights are still microscopic, clip them to a minimum\n",
    "        # This ensures the graph doesn't mathematically sever.\n",
    "        weights[weights < 1e-4] = 1e-4\n",
    "\n",
    "        sim_matrix[i, neighbors] = weights\n",
    "\n",
    "    # 4. Symmetrize (Maximum preserves the strongest connection)\n",
    "    sim_matrix = np.maximum(sim_matrix, sim_matrix.T)\n",
    "    np.fill_diagonal(sim_matrix, 0)\n",
    "\n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "sim_matrix = distance_to_local_scaled_similarity(\n",
    "    distance_matrix, k=15, scale_factor=2.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc00be14",
   "metadata": {},
   "source": [
    "### Spectral Embedding + k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2bd1ff",
   "metadata": {},
   "source": [
    "Run spectral clustering: load cached results or compute Laplacian, eigenvectors, k-means labels, and clustering quality metrics, then save everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f175c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# File to save/load clustering results\n",
    "p_clusters = Path(\"../data/ctms_clusters.pkl\")\n",
    "k_values = range(2, 15)\n",
    "\n",
    "if p_clusters.exists():\n",
    "    print(\"Clustering results already exist; loading from file.\")\n",
    "    with open(p_clusters, \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "    labels_all = results[\"labels_all\"]\n",
    "    scores = results[\"silhouette_scores\"]\n",
    "    dbi_scores = results[\"dbi_scores\"]\n",
    "    mod_scores = results[\"modularity_scores\"]\n",
    "    X_norm_all = results.get(\"X_norm_all\", None)  # load if saved\n",
    "else:\n",
    "    from scipy.linalg import eigh\n",
    "\n",
    "    # Compute Laplacian\n",
    "    D = np.diag(sim_matrix.sum(axis=1))\n",
    "    D_inv_sqrt = fractional_matrix_power(D, -0.5)\n",
    "    L_sym = np.eye(sim_matrix.shape[0]) - D_inv_sqrt @ sim_matrix @ D_inv_sqrt\n",
    "\n",
    "    scores = []\n",
    "    dbi_scores = []\n",
    "    avg_dispersion = []\n",
    "    labels_all = {}  # store labels for each k\n",
    "    X_norm_all = {}  # store normalized eigenvectors for each k\n",
    "\n",
    "    for k in k_values:\n",
    "        eigvals, eigvecs = eigh(L_sym)\n",
    "        X = eigvecs[:, :k]\n",
    "        X_norm = normalize(X, norm=\"l2\", axis=1)\n",
    "        X_norm_all[k] = X_norm  # save normalized vectors\n",
    "\n",
    "        labels = KMeans(n_clusters=k, random_state=42).fit_predict(X_norm)\n",
    "        labels_all[k] = labels\n",
    "\n",
    "        # silhouette\n",
    "        sill = manual_silhouette_score(X_norm, labels)\n",
    "        scores.append(sill)\n",
    "\n",
    "        # Davies–Bouldin Index (lower is better)\n",
    "        dbi = manual_dbi(X_norm, labels)\n",
    "        dbi_scores.append(dbi)\n",
    "\n",
    "        # average dispersion\n",
    "        dispersion = manual_average_dispersion(X_norm_all[k], labels_all[k])\n",
    "        avg_dispersion.append(dispersion)\n",
    "\n",
    "    # save results including X_norm_all\n",
    "    with open(p_clusters, \"wb\") as f:\n",
    "        pickle.dump(\n",
    "            {\n",
    "                \"labels_all\": labels_all,\n",
    "                \"silhouette_scores\": scores,\n",
    "                \"dbi_scores\": dbi_scores,\n",
    "                \"average_dispersion_scores\": avg_dispersion,\n",
    "                \"X_norm_all\": X_norm_all,\n",
    "            },\n",
    "            f,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3922f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create the figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 1. Silhouette (Higher is Better) - Solid Green\n",
    "plt.plot(k_values, scores, \"g--s\", linewidth=2, label=\"Silhouette (Max is better)\")\n",
    "\n",
    "# 2. DBI (Lower is Better) - Dashed Red\n",
    "plt.plot(k_values, dbi_scores, \"r--o\", linewidth=2, label=\"DBI (Min is better)\")\n",
    "\n",
    "# 3. Dispersion (Lower is Better) - Dashed Blue\n",
    "plt.plot(\n",
    "    k_values, avg_dispersion, \"b--^\", linewidth=2, label=\"Dispersion (Min is better)\"\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"best\", frameon=True)\n",
    "\n",
    "# Mark k=8:\n",
    "try:\n",
    "    idx_k8 = list(k_values).index(8)\n",
    "except ValueError:\n",
    "    # fallback if k_values is not a plain sequence\n",
    "    idx_k8 = int(np.where(np.array(list(k_values)) == 8)[0][0])\n",
    "\n",
    "y_sil_k8 = scores[idx_k8]\n",
    "y_dbi_k8 = dbi_scores[idx_k8]\n",
    "y_disp_k8 = avg_dispersion[idx_k8]\n",
    "\n",
    "# Vertical line at k=8\n",
    "plt.axvline(x=8, color=\"gray\", linestyle=\"--\", linewidth=1.5, label=\"k = 8\")\n",
    "\n",
    "# Horizontal lines at the three metric values at k=8\n",
    "plt.hlines(\n",
    "    y_sil_k8,\n",
    "    xmin=min(k_values),\n",
    "    xmax=max(k_values),\n",
    "    colors=\"green\",\n",
    "    linestyles=\":\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"Silhouette @k=8 ({y_sil_k8:.3f})\",\n",
    ")\n",
    "plt.hlines(\n",
    "    y_dbi_k8,\n",
    "    xmin=min(k_values),\n",
    "    xmax=max(k_values),\n",
    "    colors=\"red\",\n",
    "    linestyles=\":\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"DBI @k=8 ({y_dbi_k8:.3f})\",\n",
    ")\n",
    "plt.hlines(\n",
    "    y_disp_k8,\n",
    "    xmin=min(k_values),\n",
    "    xmax=max(k_values),\n",
    "    colors=\"blue\",\n",
    "    linestyles=\":\",\n",
    "    linewidth=1.5,\n",
    "    label=f\"Dispersion @k=8 ({y_disp_k8:.3f})\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of Clusters (k)\", fontsize=12)\n",
    "plt.ylabel(\"Score Value\", fontsize=12)\n",
    "plt.title(\"Cluster Validation Metrics Comparison\", fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ea4f2",
   "metadata": {},
   "source": [
    "### Pick the best $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3826c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = 8\n",
    "best_labels = labels_all[best_k]\n",
    "print(\"Chosen k:\", best_k)\n",
    "print(\n",
    "    f\"Silhouette score for k={best_k}: {manual_silhouette_score(X_norm_all[best_k], best_labels):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51235c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure matrices_df exists (build from df if missing)\n",
    "if \"matrices_df\" not in globals():\n",
    "    if \"ctm_matrix\" in df.columns:\n",
    "        matrices_df = df[[\"ctm_matrix\"]].rename(columns={\"ctm_matrix\": \"ctm\"}).copy()\n",
    "    elif \"ctm\" in df.columns:\n",
    "        matrices_df = df[[\"ctm\"]].copy()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Cannot build matrices_df: neither 'ctm_matrix' nor 'ctm' found in df.\"\n",
    "        )\n",
    "    matrices_df.index.name = \"song_idx\"\n",
    "\n",
    "# Choose labels: prefer best_labels, fall back to other globals if necessary\n",
    "if \"best_labels\" in globals():\n",
    "    labels = best_labels\n",
    "elif \"labels\" in globals():\n",
    "    labels = labels\n",
    "elif \"labels_all\" in globals() and \"best_k\" in globals():\n",
    "    labels = labels_all[best_k]\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"No clustering labels found in globals (expected 'best_labels' or 'labels').\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8387ebbe",
   "metadata": {},
   "source": [
    "Attach final cluster labels to the dataframe, safely serialize array columns, write the updated parquet file, and print sample items per cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd579db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align labels with how clustering was stored and write back to the original parquet\n",
    "if \"labels\" in globals():\n",
    "    labels_array = np.asarray(labels)\n",
    "elif \"best_k\" in globals() and \"labels_all\" in globals():\n",
    "    labels_array = np.asarray(labels_all[best_k])\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"No clustering labels found ('labels' or 'labels_all' with 'best_k').\"\n",
    "    )\n",
    "\n",
    "# Sanity check\n",
    "if len(labels_array) != len(df):\n",
    "    raise ValueError(f\"Length mismatch: labels ({len(labels_array)}) vs df ({len(df)})\")\n",
    "\n",
    "# Add cluster column to the dataframe\n",
    "df[\"cluster_number\"] = labels_array.astype(int)\n",
    "\n",
    "# Write the parquet with the new column\n",
    "p_path = Path(\"../data\") / \"sequence_dataset_clusters.parquet\"\n",
    "\n",
    "# Module-level import at top of cell\n",
    "import pyarrow as pa\n",
    "\n",
    "# PyArrow cannot directly convert 2D numpy arrays in a pandas column.\n",
    "# Convert the 'ctm_matrix' column to nested Python lists (serializable) or drop it\n",
    "# from the table if you don't want to save it.\n",
    "df_to_save = df.copy()\n",
    "\n",
    "if \"ctm_matrix\" in df_to_save.columns:\n",
    "    # Convert numpy arrays (129x129) to nested lists so PyArrow can handle them.\n",
    "    df_to_save[\"ctm_matrix_serialized\"] = df_to_save[\"ctm_matrix\"].apply(\n",
    "        lambda m: m.tolist()\n",
    "        if isinstance(m, np.ndarray)\n",
    "        else (m if isinstance(m, list) else None)\n",
    "    )\n",
    "    # It's safer to drop the original numpy-array column to avoid Arrow errors.\n",
    "    df_to_save = df_to_save.drop(columns=[\"ctm_matrix\"])\n",
    "\n",
    "# Build PyArrow table and write parquet (pq was imported earlier in the notebook)\n",
    "table = pa.Table.from_pandas(df_to_save, preserve_index=False)\n",
    "pq.write_table(table, p_path)\n",
    "print(f\"Saved parquet with new column 'cluster_number' to {p_path}\")\n",
    "\n",
    "# Print representative titles (or indices if 'title' missing) per cluster\n",
    "num_clusters = int(labels_array.max()) + 1\n",
    "for cluster_id in range(num_clusters):\n",
    "    mask = df[\"cluster_number\"] == cluster_id\n",
    "    n = int(mask.sum())\n",
    "    if \"title\" in df.columns:\n",
    "        items = df.loc[mask, \"title\"].head(10).values\n",
    "    else:\n",
    "        items = df.index[mask][:10].tolist()\n",
    "    print(f\"\\nCluster {cluster_id + 1} (n={n}):\")\n",
    "    print(items)\n",
    "\n",
    "# Visualize again head of parquet with new cluster_number column\n",
    "display(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5c7d2",
   "metadata": {},
   "source": [
    "### External Validation and graph visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8b7e54",
   "metadata": {},
   "source": [
    "Visualize the graphs using the forceatlas2 library and save them in the results folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4dcc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.external_validation import plot_merged_attribute_heatmap\n",
    "\n",
    "plot_merged_attribute_heatmap(df, labels_all[best_k], top_n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa03fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_visualization import visualize_graph\n",
    "\n",
    "visualize_graph(\n",
    "    sim_matrix, labels_all[best_k], filename=\"CTM_based_graph_visualization.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d873159",
   "metadata": {},
   "source": [
    "# Embeddings-based Graph Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d25fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = load_embeddings_dataset(\n",
    "    \"../data/videogame_embeddings/embedding_dataset.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mahalanobis import mahalanobis_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47653bb7",
   "metadata": {},
   "source": [
    "Compute cosine similarity matrices for all types of embeddings, after reducing the dimensions using PCA and filtering outliers with respect to a threshold using Mahalanobis distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ab390",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_cols = [\"ast\", \"wavlm\", \"clap\"]\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "game_names = embeddings_df[\"name\"].values\n",
    "\n",
    "# Dictionary for similarity matrices\n",
    "similarity_matrices = {}\n",
    "maskings = {}\n",
    "\n",
    "for col in embedding_cols:\n",
    "    embeddings = np.stack(embeddings_df[col].values)\n",
    "\n",
    "    mask, _, _ = mahalanobis_mask(reduce_dim(embeddings), alpha=0.001)\n",
    "\n",
    "    maskings[col] = mask\n",
    "\n",
    "    X_pca = reduce_dim(embeddings)\n",
    "\n",
    "    embeddings = X_pca[mask]\n",
    "\n",
    "    print(f\"Computing cosine similarity for {col}...\")\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    np.fill_diagonal(sim_matrix, 0)\n",
    "\n",
    "    similarity_matrices[col] = sim_matrix.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c18b53",
   "metadata": {},
   "source": [
    "Get the 15 closest neighbors for each node, for sparsifying the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a55ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neighbors = 15\n",
    "\n",
    "for k in list(similarity_matrices.keys()):\n",
    "    similarity_matrices[k] = to_knn_similarity(similarity_matrices[k], num_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbff78c",
   "metadata": {},
   "source": [
    "Check Sillouette scores for different clustering groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f352f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect silhouette scores for plotting\n",
    "sil_scores_collection = {}\n",
    "\n",
    "# From k=2 to k=14, test silhouette scores\n",
    "min_k = 2\n",
    "max_k = 15\n",
    "k_ranges = [r for r in range(min_k, max_k)]\n",
    "\n",
    "for col in embedding_cols:\n",
    "    print(f\"Silhouette scores for {col}...\")\n",
    "    s_matrix = similarity_matrices[col]\n",
    "\n",
    "    for k_test in k_ranges:\n",
    "        if col not in sil_scores_collection:\n",
    "            [col] = []\n",
    "\n",
    "        labels, X_norm = spectral_clustering(s_matrix, k=k_test)\n",
    "        sil_score = manual_silhouette_score(X_norm, labels)\n",
    "\n",
    "        print(f\"k = {k_test}: {sil_score:.4f}\")\n",
    "\n",
    "        sil_scores_collection[col].append(sil_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6837a9f",
   "metadata": {},
   "source": [
    "Inspect the difference of silhouette scores for a range of values for the number of clusters, for each type of embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5494b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for key, array in sil_scores_collection.items():\n",
    "    plt.plot(k_ranges, array, label=key, marker=\"o\", linewidth=2)\n",
    "\n",
    "plt.legend(title=\"Keys\", fontsize=10, title_fontsize=12, loc=\"best\")\n",
    "plt.xlabel(\"Index (time step, node, etc.)\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Comparison of arrays\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c45c1",
   "metadata": {},
   "source": [
    "Run the spectral clustering algorithm in order to obtain clusters for each graph (corresponding to each embedding type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ee9442",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 8\n",
    "embedding_labels = {}\n",
    "\n",
    "for col in embedding_cols:\n",
    "    s_matrix = similarity_matrices[col]\n",
    "    labels, X_norm = spectral_clustering(s_matrix, k=8)\n",
    "    embedding_labels[col] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff36532c",
   "metadata": {},
   "source": [
    "### External Validation and graph visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e82bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in embedding_cols:\n",
    "    s_matrix = similarity_matrices[col]\n",
    "    label = embedding_labels[col]\n",
    "    filename = col\n",
    "\n",
    "    visualize_graph(s_matrix, label, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14652ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from loguru import logger\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from typing import Dict, List, Optional, Any, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = clean_metadata_column(embeddings_df)\n",
    "ast_df = df_metadata[maskings[\"ast\"]]\n",
    "ast_df[\"cluster_label\"] = embedding_labels[\"ast\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835869a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_wise_summary(\n",
    "    df: pd.DataFrame,\n",
    "    columns: List[str],\n",
    "    save_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    For each cluster and each column:\n",
    "    - Count occurrences of each unique element.\n",
    "    - Fully supports list-like columns (themes, keywords, companies).\n",
    "    - Saves a JSON summary of value frequencies.\n",
    "\n",
    "    Output format:\n",
    "    {\n",
    "        \"0\": {\n",
    "            \"themes\": {\"Action\": 10, \"Sci-fi\": 4, ...},\n",
    "            \"keywords\": {...},\n",
    "            ...\n",
    "        },\n",
    "        \"1\": { ... },\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    clusters = sorted(df[\"cluster_label\"].unique())\n",
    "\n",
    "    summary: Dict[Any, Dict[str, Dict[str, int]]] = {}\n",
    "\n",
    "    for c in clusters:\n",
    "        # Convert cluster label to JSON-serializable type\n",
    "        c_key = str(int(c)) if isinstance(c, (np.integer, np.floating)) else str(c)\n",
    "        subset = df[df[\"cluster_label\"] == c]\n",
    "        cluster_num = len(subset)\n",
    "        \n",
    "        summary[c_key] = {\n",
    "            col: {} for col in columns\n",
    "        }\n",
    "        summary[c_key][\"cluster_size\"] = cluster_num\n",
    "        \n",
    "        for col in columns:\n",
    "            logger.debug(f\"Processing cluster {c_key}, column '{col}'…\")\n",
    "            subset = df[df[\"cluster_label\"] == c]\n",
    "\n",
    "            # Ensure the nested dict for this column exists\n",
    "            if col not in summary[c_key]:\n",
    "                summary[c_key][col] = {}\n",
    "\n",
    "            if subset.empty:\n",
    "                logger.warning(f\"Cluster {c_key} has no entries for column '{col}'.\")\n",
    "                # Keep as empty dict for empty subsets\n",
    "                continue\n",
    "            \n",
    "\n",
    "            # Flatten list values OR handle scalar values\n",
    "            flat_values = []\n",
    "            for item in subset[col]:\n",
    "                if isinstance(item, str):\n",
    "                    # Try to parse JSON string\n",
    "                    try:\n",
    "                        parsed = ast.literal_eval(item)\n",
    "                        if isinstance(parsed, list):\n",
    "                            flat_values.extend(parsed)\n",
    "                        else:\n",
    "                            flat_values.append(parsed)\n",
    "                    except (ValueError, SyntaxError):\n",
    "                        # Not JSON, treat as plain string\n",
    "                        flat_values.append(item)\n",
    "                elif isinstance(item, list):\n",
    "                    flat_values.extend(item)\n",
    "                else:\n",
    "                    flat_values.append(item)\n",
    "\n",
    "            counts = dict(Counter(flat_values))\n",
    "            # Convert keys and values to JSON-serializable types\n",
    "            counts = {str(k): int(v) for k, v in counts.items() if isinstance(v, (int, np.integer))}\n",
    "            # Sort by count (descending)\n",
    "            counts = dict(sorted(counts.items(), key=lambda x: -x[1]))\n",
    "            # Store counts under a dedicated key to avoid overwriting\n",
    "            summary[c_key][col][\"counts\"] = counts\n",
    "\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a1d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_wise_summary(ast_df, [\"name\", \"themes\", \"keywords\", \"involved_companies\", \"first_release_year\"], \"../results/embedding_general/graph_clustering/gc.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73896ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_cluster_summary(\n",
    "    df: pd.DataFrame,\n",
    "    save_path: Optional[str] = None,\n",
    "    k: int = 20,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df: int = 1,\n",
    "    max_df: float = 0.8,\n",
    "    scoring: str = \"distinctiveness\",  # \"distinctiveness\" or \"tfidf\"\n",
    ") -> Dict[Union[int, str], List[str]]:\n",
    "    \"\"\"\n",
    "    Compute top-k keywords per cluster from the 'metadata' column.\n",
    "\n",
    "    Uses a GLOBAL TF-IDF fit and selects terms that are\n",
    "    distinctive to each cluster via:\n",
    "        score = mean_tfidf_in_cluster - mean_tfidf_global\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing 'cluster_label' and 'metadata' (or 'metadata_combined').\n",
    "        k: Top-k terms to return per cluster.\n",
    "        ngram_range: N-gram range for TF-IDF.\n",
    "        min_df: Minimum doc frequency (absolute).\n",
    "        max_df: Max document frequency ratio.\n",
    "        scoring: 'distinctiveness' (default) or 'tfidf' (cluster mean TF-IDF).\n",
    "    \"\"\"\n",
    "\n",
    "    if \"cluster_label\" not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'cluster_label'.\")\n",
    "\n",
    "    if \"metadata\" in df.columns:\n",
    "        text_col = \"metadata\"\n",
    "    elif \"metadata_combined\" in df.columns:\n",
    "        text_col = \"metadata_combined\"\n",
    "    else:\n",
    "        raise ValueError(\"DataFrame must contain 'metadata' or 'metadata_combined'.\")\n",
    "\n",
    "    all_texts = df[text_col].fillna(\"\").astype(str).tolist()\n",
    "    if len(all_texts) == 0 or all(t.strip() == \"\" for t in all_texts):\n",
    "        return {}\n",
    "\n",
    "    n_docs = len(all_texts)\n",
    "\n",
    "    # Ensure max_df is consistent with min_df for small n_docs\n",
    "    if isinstance(max_df, float) and max_df < 1.0:\n",
    "        min_allowed_max_df = min_df / n_docs\n",
    "        if max_df < min_allowed_max_df:\n",
    "            max_df = min_allowed_max_df\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=ngram_range,\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        tfidf_all = vectorizer.fit_transform(all_texts)\n",
    "    except ValueError as e:\n",
    "        # Usually \"empty vocabulary\" – relax constraints\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            lowercase=True,\n",
    "            stop_words=None,\n",
    "            ngram_range=ngram_range,\n",
    "            min_df=1,\n",
    "            max_df=1.0,\n",
    "        )\n",
    "        tfidf_all = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "    if tfidf_all.shape[1] == 0:\n",
    "        # Still nothing – bail out gracefully\n",
    "        return {}\n",
    "\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "    global_mean = np.asarray(tfidf_all.mean(axis=0)).ravel()\n",
    "\n",
    "    results: Dict[Union[int, str], List[str]] = {}\n",
    "    clusters = df[\"cluster_label\"].unique()\n",
    "\n",
    "    for cluster in clusters:\n",
    "        idx = np.where(df[\"cluster_label\"].values == cluster)[0]\n",
    "        if len(idx) == 0:\n",
    "            results[cluster] = []\n",
    "            continue\n",
    "\n",
    "        tfidf_cluster = tfidf_all[idx, :]\n",
    "        if tfidf_cluster.shape[0] == 0:\n",
    "            results[cluster] = []\n",
    "            continue\n",
    "\n",
    "        cluster_mean = np.asarray(tfidf_cluster.mean(axis=0)).ravel()\n",
    "\n",
    "        if scoring == \"tfidf\":\n",
    "            scores = cluster_mean\n",
    "        else:\n",
    "            scores = cluster_mean - global_mean\n",
    "\n",
    "        if scores.size == 0:\n",
    "            results[cluster] = []\n",
    "            continue\n",
    "\n",
    "        top_idx = np.argsort(scores)[::-1][:k]\n",
    "        results[cluster] = feature_names[top_idx].tolist()\n",
    "\n",
    "    # JSON can't handle np.int64 keys\n",
    "    results = {int(c): v for c, v in results.items()}\n",
    "\n",
    "    if save_path:\n",
    "        logger.info(f\"Saving TF-IDF cluster summary to {save_path}\")\n",
    "        with open(save_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972fe719",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_cluster_summary(ast_df, \"../results/embedding_general/graph_clustering/gc_all.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
